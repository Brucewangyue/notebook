# 流量负载



K8S的流量负载组件：

- Service：4层负载
- Ingress：7层负载



## Service

![image-20210905153519032](assets/image-20210905153519032.png)

> **kube-proxy**
>
> ​    Service很多情况下只是一个概念，真正起作用的是`kube-proxy`服务进程，每个node节点上都运行着一个`kube-proxy`服务进程，当创建Service的时候通过api-server向etcd写入创建的service信息，而`kube-proxy`会基于监听的机制发现这种service的变化，然后它会将最新的service信息转换成对应的访问规则

![image-20210906105909064](assets/image-20210906105909064.png)

![image-20210906110657769](assets/image-20210906110657769.png)

kube-proxy支持三种工作模式：

- **userspace模式**

  - kube-proxy会为每一个service创建一个监听端口，发向clusterIP的请求被iptables（在内核空间）规则重定向到kube-proxy监听的端口上，kube-proxy根据LB算法选择一个提供服务的pod并和其建立连接，以将请求转发到pod上。
  - 该模式下，kube-proxy充当于一个4层的负载均衡器的角色。由于kube-proxy运行在userspace(用户空间)中，在进行转发处理时增加内核和用户空间的数据拷贝，虽然稳定，但是效率较低。
  - ![image-20210906110920574](assets/image-20210906110920574.png)

- **iptables模式（默认）**

  - kube-proxy为service后端的每个pod创建对应的iptables规则，直接将clusterIP的请求重定向到一个podIP
  - iptables的LB策略不够灵活，没有重试机制
  - ![image-20210906111533076](assets/image-20210906111533076.png)

- **ipvs模式（最佳）**

  - ipvs的转发效率比iptables更高，ipvs支持更多的LB算法

  - ![image-20210906111940537](assets/image-20210906111940537.png)

  - ```sh
    # 此模式必须安装ipvs内核模块
    # 开启ipvs
    $ kubectl edit cm kube-proxy -n kube-system
    # 修改配置项：mode: "ipvs"
    
    # 删除当前的pod让他重建
    $ kubectl delete pod -l k8s-app=kube-proxy -n kube-system
    # -l k8s-app=kube-proxy 按标签k8s-app=kube-proxy来选择
    
    # 查看
    ipvsadm -Ln
    ```

### 使用

#### 资源清单

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-deploy-nginx
  labels:
    app: deploy-nginx
spec:
  selector:
    app: deploy-nginx
  ports:
  - name: http
    protocol: TCP
    port: 8080       # serivce自己的端口
    targetPort: 80   # 代理的Pod的端口（后端应用的端口）
  - name: https
    port: 443
    protocol: TCP
    targetPort: 443
```

#### Endpoints

每创建一个Service就会自动创建一个跟Service同名的Endpoints，Endpoints管理了所有匹配的Pod的IP端口

EndPoint会监听它所管理Pod的重建，同步新Pod的IP

```sh
# 查询EndPoint
$ kubectl get ep 
```



#### 代理K8s外部的应用

希望在生产环境中使用某个固定的名称而非IP地址进行访问外部的中间件服务，相当于对外部应用做了一个映射，然后内部应用使用服务名称来访问，防止外部中间件因地址改动而影响内部应用的稳定，如：旧服务迁移到新K8s过渡阶段，某些服务未来得及迁移

```sh
# 创建Service (不配置selector，需要手动创建ep)
------------ svc-external.yml
apiVersion: v1
kind: Service
metadata:
  name: svc-external
  labels:
    app: svc-external
spec:
  ports:
  - name: http
    protocol: TCP
    port: 8080       # serivce自己的端口
    targetPort: 80   # 代理的Pod的端口（后端应用的端口）
-----------------
$ kubectl apply -f svc-external.yml 

# 创建Endpoints
------------- svc-ep-external.yml
apiVersion: v1
kind: Endpoints
metadata:
  name: svc-external      # 跟service同名，会自动建立联系
  labels:
    app: svc-external
subsets:
- addresses:
  - ip: 220.181.38.148     # 外部应用
  ports:
  - name: http
    port: 80
---------------
$ kubectl apply -f svc-ep-external.yml

# 查看serviceIP
$ kubectl get svc,ep

# 测试
$ curl 10.106.127.232 -I
HTTP/1.1 200 OK
...

$ curl baidu.com -I
HTTP/1.1 200 OK
...

```



#### 反代外部域名

ep都不用配置

```sh
apiVersion: v1
kind: Service
metadata:
  name: svc-external-name
  labels:
    app: svc-external-name
spec:
  type: ExternalName
  externalName: www.baidu.com
```

> 注意：由于没有IP，需要配置CoreDNS才能访问
>
> Pod使用的是CoreDNS，所有我们可以进入一个busybox的Pod中测试
>
> `k8s-yml]# kubectl exec -it busybox -- sh`
>
> ```sh
> wget svc-external-name
> Connecting to svc-external-name (163.177.151.110:80)
> wget: server returned error: HTTP/1.1 403 Forbidden
> 
> # 这里反代出现了跨域，被百度拒绝
> ```



### 类型

```yaml
kind: Service
metadata:
  name: service-name
  namespace: dev
spec:
  selector:
    app: deploy-nginx
  type:  # Service类型，制定Service的访问方式
  clusterIP: # 虚拟服务的ip地址，如果不写默认分配
  sessionAffinity: # session亲和性，支持ClusterIP、None两个选项。ClusterIP会让同一个ip访问固定的pod
  ports:
  - port: 3017 # service端口
    targetPort: 5003 # pod端口
    nodePort: 31192 # 主机端口
```

type的类型：

- ClusterIP：只能在集群内访问
- NodePort：在集群外部访问内部服务
- LoadBalancer：
- ExternalName：
- headLess：无头类型，将type设置成ClusterIP，然后clusterIP设置成None

#### **HeadLess**

用于自定义负载策略

```sh
# 随机进入一台服务内的pod
$ kubectl exec -it pod-name -n dev /bin/bash

# 查看默认域名解析器
$ cat /etc/resolv.conf

# 
dig @默认域名 service名称.命名空间.svc.cluster.local
```

#### **NodePort**

将service的端口映射到node的端口上

![image-20210906122859792](assets/image-20210906122859792.png)

#### **LoadBalancer**

跟NodePort类似，目的都是向外暴露一个端口，区别在于LoadBalancer会在集群外部再做一个负载均衡设备，而这个设备需要外部环境的支持，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中

![image-20210906122934206](assets/image-20210906122934206.png)

#### **ExternalName**

![image-20210906123158104](assets/image-20210906123158104.png)



## Ingress

### 环境搭建

```sh
$ mkdir ingress-controller
$ cd ingress-controller

# 下载ingress-nginx
$ wget .../mandatory.yaml
$ wget .../service-nodeport.yaml

# 修改仓库镜像源

# 创建ingress-nginx
$ kubectl apply -f ./

# 查看ingress-nginx
$ kubectl get po -n ingress-nginx

# 查看service
$ kubectl get svc -n ingress-nginx
```



### 搭建服务

![image-20210906133434777](assets/image-20210906133434777.png)

注意：tomcat版本使用：tomcat:8.5-jre10-slim（这个版本带默认首页）

### HTTP代理 

```sh
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-http
  namespace: dev
spec:
  rules:
  - host: nginx.web.com
    http:
      paths:
      - path: /
        backend: 
          serviceName: nginx-service
          servicePort: 80
  - host: tomcat.web.com
    http:										
      paths:
      - path: /
        backend:
          serviceName: tomcat-service
          servicePort: 80
```

## 