# 集群环境搭建

## Kubeadm搭建单机集群

- 搭建测试环境集群：1主2从
- 3台2核2G内存服务器
- 系统版本：centos 8
- Docker版本：20.10.8
- K8s版本：v1.20.10

### 环境初始化

```sh
# 设置主机名后重启，主机名在创建k8s集群时会使用
$ hostnamectl set-hostname [主机名]

# 以下命令再所有机器上执行
# 主机域名配置
# 企业中推荐使用内部的DNS服务器
$ vi /etc/hosts
192.168.177.100 master1
192.168.177.101 node1
192.168.177.102 node2

# 测试是否有效
$ ping node1
PING node1 (192.168.177.101) 56(84) bytes of data.

# 时间同步
# Kubernetes集群需要服务器时间精准一至，企业中建议配置内部的时间同步服务器
# 启动chronyd服务
$ systemctl start chronyd && systemctl enable chronyd

# 测试
$ date
2021年 08月 26日 星期四 21:49:54 EDT

# 禁用iptables和firewalld服务(可选：测试环境)
# Kubernetes和Docker在运行中会产生大量的iptables规则，为了不让系统规则和它们混淆，关闭系统规则
$ systemctl stop firewalld && systemctl disable firewalld
$ systemctl stop iptables && systemctl disable iptables

# 禁用selinux  (待查证)
# selinux是linux系统下的一个安全服务，如果不关闭它，在安装集群时会产生各种各样的问题
$ vi /etc/selinux/config
SELINUX=disabled

# 重启后生效

# 禁用swap分区（虚拟内存分区）
# 它的作用是在物理内存使用完之后，将磁盘空间虚拟成内存来使用，启动swap设备会对系统的性能产生非常负面的影响，因此Kubernetes要求每个节点都要禁用swap设备，但是也可以在安装的过程中进行参数配置来允许swap分区被启用
$ vi /etc/fstab
#/dev/mapper/cl-swap     swap                    swap    defaults        0 0

# 重启后生效

# 允许 iptables 检查桥接流量，修改linux内核参数（Kubernetes要求）
# Kubernetes支持几种不同的网络附加解决方案。接下来安装flannel网络插件，我们需要做的第一件事是我们需要在所有服务器上设置一个sysctl值，包括主节点
$ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

$ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

$ sysctl --system

# 加载模块
$ modprobe br_netfilter
# 查看模块是否加载
$ lsmod | grep br_netfilter

# 配置ipvs
# Kubernetes中service有两种代理模型，iptables，ipvs，后者性能更高，需要手动载入ipvs模块
# 安装ipset,ipvsadm
$ yum install ipset ipvsadm -y
# 添加需要加载的模块写入脚本文件
$ cat << EOF >/etc/sysconfig/modules/ipvs.modules
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF
# centos8.3 找不到nf_conntrack_ipv4模块 ,更换为nf_conntrack试试
# modprobe -- nf_conntrack_ipv4

$ chmod +x /etc/sysconfig/modules/ipvs.modules
# $ modprobe nf_conntrack_ipv4
$ modprobe nf_conntrack
$ /bin/bash /etc/sysconfig/modules/ipvs.modules

# 重启服务器
$ reboot

# 查看配置是否生效
$ getenforce
Disabled
$ free -m
              total        used        free      shared  buff/cache   available
Mem:           1790         190        1371           8         228        1436
Swap:             0           0           0

```



### 安装Docker

```sh
yum install -y yum-utils
# 国内仓库
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

yum install docker-ce docker-ce-cli containerd.io

# 查看当前镜像源中支持的docker版本
yum list docker-ce --showduplicates
Docker CE Stable - x86_64     48 kB/s |  15 kB     00:00    
可安装的软件包
docker-ce.x86_64      3:19.03.13-3.el8         docker-ce-stabl
# 安装指定版本
yum install --setopt=obsoletes=0 docker-ce-19.03.15 docker-ce-cli-19.03.15 containerd.io -y
# --setopt=obsoletes=0 此参数为取消安装包取代关系，有时候有些旧的安装包被新的安装包取代了，安装的时候就会自动安装新的。

# 添加配置文件
# Docker默认情况下使用Cgroup Driver为cgroupfs，而K8s推荐使用systemd替代cgroupfs
mkdir /etc/docker
cat << EOF >/etc/docker/daemon.json
{
    "exec-opts": ["native.cgroupdriver=systemd"],
    "registry-mirrors": [
            "https://registry.aliyuncs.com"
          ]
}
EOF

# 启动Docker
systemctl start docker && systemctl enable docker
# 查看Docker信息
$ docker info 
 ...
 Cgroup Driver: systemd

```



### 安装K8s组件

你需要在每台机器上安装以下的软件包：

- `kubeadm`：用来初始化集群的指令。
- `kubelet`：在集群中的每个节点上用来启动 Pod 和容器等。
- `kubectl`：用来与集群通信的命令行工具。

```sh
# 由于K8s的镜像源在国外，速度比较慢，更换为国内镜像源
cat << EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 安装（最新版本可能会有一些组件的不兼容，如网络组件）
yum install -y kubelet kubeadm kubectl
# 查看当前镜像源中支持的docker版本
yum list kubeadm --showduplicates
# 安装指定版本
yum install -y --setopt=obsoletes=0 kubelet-1.20.10-0 kubeadm-1.20.10-0 kubectl-1.20.10-0

# 配置kubelet的cgroup
cat << EOF > /etc/sysconfig/kubelet
KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"
KUBE_PROXY_MODE="ipvs"
EOF

# 设置开机启动
$ systemctl restart kubelet && systemctl enable kubelet

# 使用kubeadm安装k8s集群时，会到docker镜像仓库下载k8s所需组件的镜像
# 查看需要下载的镜像
$ kubeadm config images list

I0830 02:12:32.470444    8646 version.go:254] remote version is much newer: v1.22.1; falling back to: stable-1.20
k8s.gcr.io/kube-apiserver:v1.20.10
k8s.gcr.io/kube-controller-manager:v1.20.10
k8s.gcr.io/kube-scheduler:v1.20.10
k8s.gcr.io/kube-proxy:v1.20.10
k8s.gcr.io/pause:3.2
k8s.gcr.io/etcd:3.4.13-0
k8s.gcr.io/coredns:1.7.

# 由于k8s.gcr.io国内无法访问，所以需要替换镜像源

# 拉取镜像
# 方法一
$ kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers
# 如果中途找不到最新的 coredns，那么就单独下载latest版本，修改名字为指定的版本

# 下载
$ docker pull registry.aliyuncs.com/google_containers/coredns:latest
# 改名
$ docker tag registry.aliyuncs.com/google_containers/coredns:latest registry.aliyuncs.com/google_containers/coredns:v1.8.4

# 方法二
$ images=(
    kube-apiserver:v1.17.17
    kube-controller-manager:v1.17.17
    kube-scheduler:v1.17.17
    kube-proxy:v1.17.17
    pause:3.1
    etcd:3.4.3-0
    coredns:1.6.5
)

for image in ${images[@]}
do
	docker pull registry.aliyuncs.com/google_containers/${image}
	docker tag registry.aliyuncs.com/google_containers/${image} k8s.gcr.io/${image}
	docker image rm registry.aliyuncs.com/google_containers/${image}
done
```



### 集群初始化

```sh
# 下面的操作只需要在master节点上执行
# 创建集群，会自动下载kubernetes所需的组件
$ kubeadm init \
--image-repository registry.aliyuncs.com/google_containers \
--pod-network-cidr=10.244.0.0/16 \
--apiserver-advertise-address=192.168.177.111 \
--kubernetes-version=v1.20.10 \
--node-name=master1

#   --node-name（重点）：如果执行当前命令的主机名不是域名，需要指定node-name参数
#   --kubernetes-version：kubernetes版本
#   --pod-network-cidr：指明 pod 网络可以使用的 IP 地址段。如果设置了这个参数，控制平面将会为每一个节点自动分配 CIDRs
#	--apiserver-advertise-address ： 主节点Ip,为master(控制面板)节点的 API server 设置广播地址

Your Kubernetes control-plane has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
Alternatively, if you are the root user, you can run:
  export KUBECONFIG=/etc/kubernetes/admin.conf
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/
Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 192.168.177.100:6443 --token lsnex0.2tnwvfj3kl2c0ziz \
	--discovery-token-ca-cert-hash sha256:479d6791a9108e17a6e04d405ae95564c414acb756bd13043ab5a5910412dd0a 
#############

# 创建集群成功后，按提示创建目录和文件

# 创建集群加入令牌
$ kubeadm token create
# 其他节点加入集群
# kubeadm join [api-server-endpoint] [flags]
$ kubeadm join 192.168.177.100:6443 \
--discovery-token eeqqgv.ytlyhleqb5i7w4v4 \
--discovery-token-ca-cert-hash sha256:1a78b6956df07f02d330505afa7c299bd339b6d8ac2c3e36539e6ec2f6554419

# 查看集群状态
$ kubectl get nodes
NAME     STATUS     ROLES                  AGE     VERSION
master   NotReady   control-plane,master   19m     v1.22.1
node1    NotReady   <none>                 7m38s   v1.22.1
node2    NotReady   <none>                 11m     v1.22.1

# 可以看到所有节点的状态都为未准备，是因为还未安装网络插件
```



### 安装网络插件

K8s支持多种网络插件，比如flannel、calico、canal等，以下使用[flannel](https://github.com/flannel-io/flannel#flannel)

```sh
# 以下操作再master节点执行
$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

# 修改文件中的quay.io仓库为quay-mirror.qiniu.com

# 使用配置文件启动flannel
$ kubectl apply -f flannel.yml

# 再次查看集群状态
$ kubectl get po -n kube-system

NAME                               READY   STATUS              RESTARTS   AGE
coredns-7f89b7bc75-mg85w           0/1     Pending             0          11m
coredns-7f89b7bc75-twgb2           0/1     Pending             0          11m
etcd-master11                      1/1     Running             0          12m
kube-apiserver-master11            1/1     Running             0          12m
kube-controller-manager-master11   1/1     Running             0          12m
kube-flannel-ds-6ftfn              0/1     Init:ErrImagePull   0          3m19s
kube-proxy-7lm2m                   1/1     Running             0          11m
kube-scheduler-master11            1/1     Running             0          12m

# 发现kube-flannel-ds-6ftfn 镜像无法拉取，问题出在了flannel安装文件中的镜像源
# 在dockerhub中查找flannel，使用一个靠前的镜像仓库，然后替换即可
# 删除原来的配置
$ kubectl delete -f flannel.yml
# 修改好flannel.yml文件中的镜像源后再安装
```

![image-20210831161752947](assets/image-20210831161752947.png)

![image-20210831161827567](assets/image-20210831161827567.png)

```sh
# 配置完等待1分钟
$ kubectl get po -n kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-7f89b7bc75-mg85w           1/1     Running   0          32m
coredns-7f89b7bc75-twgb2           1/1     Running   0          32m
etcd-master11                      1/1     Running   0          32m
kube-apiserver-master11            1/1     Running   0          32m
kube-controller-manager-master11   1/1     Running   0          32m
kube-flannel-ds-rdn56              1/1     Running   0          108s
kube-proxy-7lm2m                   1/1     Running   0          32m
kube-scheduler-master11            1/1     Running   0          32m

```



### 安装DashBoard

[官方安装教程](https://kubernetes.io/zh/docs/tasks/access-application-cluster/web-ui-dashboard/)

注意：国内用户搭建的时候替换yaml中的镜像即可

**访问**

```sh
# 创建账号
$ kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard

# 授权
$ kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin

# 获取账号token
$ kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin

# 得到 secretvalue

# 获取token
$ kubectl describe secret secretvalue -n kubernetes-dashboard
```



### 安装异常处理

- 删除集群、退出已加入的集群

  ```sh
  kubeadm reset -f
  ```

- 卸载集群，重新创建集群后提示证书不正确

  ```sh
  $ kubectl get nodes
  
  Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
  
  # 处理办法：删除目录后重新创建集群
  $ rm -rf $HOME/.kube
  ```

- kubectl get nodes无法获取信息

  ```sh
  [root@master ~]# kubectl get nodes
  The connection to the server localhost:8080 was refused - did you specify the right host or port?
  
  $ mkdir -p $HOME/.kube
  $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  $ sudo chown $(id -u):$(id -g) $HOME/.kube/config
  ```



### 测试

```sh
# 部署nginx
$ kubectl create deployment nginx --image=nginx

# 暴露端口
$ kubectl expose deployment nginx --port=80 --type=NodePort

# 查看服务状态
$ kubectl get pod,svc

# 查看pod的详细信息，包括创建过程
$ kubectl describe pod podname
```

  

## Kubeadm搭建高可用集群

![image-20210908102657992](assets/image-20210908102657992.png)

- 搭建测试环境集群：3主2从
- 5台2核2G服务器
- 系统版本：centos 8.4
- 内核版本：4.18
- Docker版本：19.03.15
- K8s版本：v1.20.10

### 环境初始化

```sh
# 设置主机名
$ hostnamectl set-hostname k8s-master1
...

# 如果是通过VMware克隆的节点，修改机器id，否则可能会出现ipv6重复的错误
$ cp /etc/machine-id /etc/machine-id-bak
$ rm /etc/machine-id
$ systemd-machine-id-setup

# 配置域
$ vi /etc/hosts

192.168.177.100 k8s-master1
192.168.177.101 k8s-master2
192.168.177.102 k8s-master3
192.168.177.236 k8s-master-lb # vip（任意一个同网段，公司内网IP不重复，PING不通的IP）
192.168.177.103 k8s-node1
192.168.177.104 k8s-node2

# 关闭防火墙、
$ systemctl stop firewalld && systemctl disable firewalld

# 关闭 dnsmasq
$ systemctl stop dnsmasq && systemctl disable dnsmasq

# （Centos8关闭后ens33网卡没了？？？）关闭 NetworkManager（如果不关闭需要额外配置，否则pod过多宿主机不稳定）
$ systemctl stop NetworkManager && systemctl disable NetworkManager

# 禁用selinux
$ vi /etc/selinux/config

SELINUX=disabled

# 禁用swap分区
$ vi /etc/fstab

#/dev/mapper/cl-swap     swap                    swap    defaults        0 0

# 安装常用工具
$ yum -y install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git

# 时间同步
--centos8----------- chronyd
$ systemctl start chronyd && systemctl enable chronyd
# 配置ntp服务端（可选？）
$ vi /etc/chrony.conf

# 解开注释
allow 192.168.0.0/16

$ systemctl restart chronyd

# 配置ntp客户端（可选？）
$ vi /etc/chrony.conf

# 加在顶部
Server  192.168.0.016

$ systemctl restart chronyd
--centos7----------- ntpdate
# 安装ntpdate
$ rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm
$ yum install ntpdate -y

# 同步阿里云时间
$ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
$ echo 'Asia/Shanghai' >/etc/timezone
$ ntpdate time2.aliyun.com

# 定时同步
$ crontab -e

*/5 * * * * ntpdate time2.aliyun.com
--------------


# 所有节点配置连接数最大值，默认1024
# 临时
$ ulimit  -SHn 65535

# 永久
$ vi /etc/security/limits.conf

# 末尾添加
* soft nofile 655360
* hard nofile 131072
* soft nproc 655350
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited


# master1节点免密登录其他节点，阿里云或者AWS上需要单独一台kubectl服务器
# 在master1上生成证书
$ ssh keygen -t rsa

# 将公钥发送到其他节点
# ssh-copy-id    # 将本地公钥拷贝到远程节点的authorized_keys文件中，并且设置远程服务器的权限
# -i             # 指定公钥文件
$ for i in k8s-master2 k8s-master3 k8s-node1 k8s-node2;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
```



### 升级系统和内核 （Centos7）

如果使用的是centos7，需要升级内核到7.9版本，避免出现意外的异常发生，如：提示磁盘空间不足，其实磁盘空间充足，宿主机重启

```sh
# 查看系统版本
$ cat /etc/redhat-release

# 升级系统
$ yum update -y  && reboot

# 查看内核版本
$ uname -a

# 升级内核到4.18+
# 网上下载指定版本的内核
# 安装内核
# 修改默认使用内核
$ grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg
$ grubby --arg="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"

# 检查版本
$ grubby --default-kernel

# 重启后进入系统内核选择界面时多了新版本内核的选择
```



### 内核配置

**ipvsadm 生产环境首选**

```sh
# 配置使用 ipvsadm 替代 iptables
$ yum install ipvsadm ipset sysstat conntrack libseccomp -y

# 配置ipvs模块
$ vim /etc/modules-load.d/ipvs.conf 

ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
#nf_conntrack_ipv4 (内核版本4.18+改为nf_conntrack)
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip

# 加载
$ systemctl enable --now systemd-modules-load.service

# 开启一些k8s集群中必须的内核参数，所有节点配置k8s内核
$ cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1

net.ipv4.ip_forward = 1
fs.may_detach_mounts = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF

$ sysctl --system

# 需要重启

# 查看
lsmod | grep --color=auto -e ip_vs -e nf_
```

**iptables （测试）**

```sh
# 允许 iptables 检查桥接流量
$ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
$ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
$sudo sysctl --system
# 加载模块
$ modprobe br_netfilter
# 查看模块是否加载
$ lsmod | grep br_netfilter

# 需要重启

# 查看
$ lsmod | grep br_netfilter
```



### 安装Docker

```sh
# 国内仓库
$ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

# 查看docker版本列表
$ yum list docker-ce --showduplicates

# 安装指定版本
$ yum install --setopt=obsoletes=0 docker-ce-19.03.15 docker-ce-cli-19.03.15 containerd.io -y

# 添加配置文件
$ mkdir /etc/docker
$ cat << EOF >/etc/docker/daemon.json
{
    "exec-opts": ["native.cgroupdriver=systemd"],
    "registry-mirrors": [
            "https://registry.aliyuncs.com"
          ]
}
EOF

# 启动Docker
$ systemctl start docker && systemctl enable docker
# 查看Docker信息
$ docker info 
```



### 安装K8s组件

```sh
# 在所有节点操作，因为主机节点要初始化、工作节点要加入主节点
# 添加K8s仓库源
$ cat << EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 查看当前镜像源中支持的docker版本
$ yum list kubeadm --showduplicates

# 安装指定版本  暂时不安装 
$ yum install -y --setopt=obsoletes=0 kubeadm-1.20.10-0 kubelet-1.20.10-0 kubectl-1.20.10-0

# 配置kubelet的cgroup
# KUBELET_CGROUP_ARGS="--pod-infra-container-image=registry.aliyuncs.com/google_containers/pause-amd64:3.2" 可以单独指定K8s组件版本
$ cat << EOF > /etc/sysconfig/kubelet
KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"
KUBE_PROXY_MODE="ipvs"
EOF

# 设置开机启动
$ systemctl restart kubelet && systemctl enable kubelet

# （在主节点上操作）拉取k8s组件镜像
$ kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers
```



### 高可用组件安装

```sh
# 在所有master节点上操作
$ yum install -y keepalived haproxy

# 配置 haproxy
$ vim /etc/haproxy/haproxy.cfg

global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

frontend k8s-master
  bind 0.0.0.0:16443
  bind 127.0.0.1:16443
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-master1	192.168.177.100:6443  check
  server k8s-master2	192.168.177.101:6443  check
  server k8s-master3	192.168.177.102:6443  check
  
# 配置 keepalive
# 注意替换成每个节点自己的IP
$ mkdir /etc/keepalived
$ vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state MASTER    # master1是MASTER 备机为 BACKUP
    interface ens33   # 这里为本机网卡名
    mcast_src_ip 192.168.177.100    # 这里为本机IP
    virtual_router_id 51
    priority 101    # master1 是101，备机为 100
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    }
    virtual_ipaddress {
        192.168.177.236
    }
    # 健康检查
    track_script {
       chk_apiserver
    }
}

# 配置 keepalive 健康检查脚本
$ vim /etc/keepalived/check_apiserver.sh 

#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == "" ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != "0" ]]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi

$ chmod +x /etc/keepalived/check_apiserver.sh
$ systemctl daemon-reload
$ systemctl start haproxy && systemctl start keepalived
$ systemctl enable haproxy && systemctl enable keepalived

# 查看16443端口是否起来
$ netstat -lntp

# 查看vip是否正常，出现 "Escape character is '^]'." 表示正常
$ telnet 192.168.177.236 16443
```



### 集群初始化

在主节点上操作

```sh
# 创建 kube-init.yaml
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: 7t2weq.bjbawausm0jaxury
  ttl: 24h0m0s     # token过期时间
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.177.100     # master1IP
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: k8s-master1                    # hostname
  taints:                               # 污点
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  certSANs:
  - 192.168.177.236   # 
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 192.168.177.236:16443   # 如果不是高可用集群，使用master1IP
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers  # 修改镜像源
kind: ClusterConfiguration
kubernetesVersion: v1.20.10     # 注意当前使用的版本
networking:
  dnsDomain: cluster.local
  podSubnet: 172.168.0.0/16     # 替换pod网段
  serviceSubnet: 10.96.0.0/12   # 替换service网段
scheduler: {}

# 更新kubeadm文件
$ kubeadm config migrate --old-config kube-init.yaml --new-config new-kube-init.yaml

# 初始化
$ kubeadm init --config new-kube-init.yaml  --upload-certs

# root用户配置使用kubectl
$ cat <<EOF >> /root/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF

$ source /root/.bashrc

# 查看节点状态
$ kubectl get nodes

# 采用初始化安装方式，所有的系统组件均以容器的方式运行并且在kube-system命名空间内，此时可以查看Pod状态
$ kubectl get pods -n kube-system -o wide
```



### 节点加入集群

```sh
# 生成token
$ kubectl token create --print-join-command

# 查看最新生成的token
$ kubectl get secret -n kube-system

# 查看token详细信息
$ kubectl get secret -n kube-system secret-name -oyaml

# master节点加入额外需要
$ kubectl init phase upload-certs --upload-certs

# master节点加入集群
kubeadm join 192.168.177.236:16443 --token 7t2weq.bjbawausm0jaxury \
--discovery-token-ca-cert-hash sha256:8c92ecb336be2b9372851a9af2c7ca1f7f60c12c68f6ffe1eb513791a1b8a908 \
--control-plane --certificate-key ac2854de93aaabdf6dc440322d4846fc230b290c818c32d6ea2e500fc930b0aa

# worker节点加入集群
kubeadm join 192.168.0.236:16443 --token 7t2weq.bjbawausm0jaxury \
--discovery-token-ca-cert-hash sha256:8c92ecb336be2b9372851a9af2c7ca1f7f60c12c68f6ffe1eb513791a1b8a908
```



### 安装Calico网络组件

```sh
# calico模板 https://github.com/dotbalo/k8s-ha-install/blob/manual-installation-v1.20.x/calico/calico-etcd.yaml

# 修改calico-etcd.yaml的以下位置

# 替换master节点IP
$ sed -i 's#etcd_endpoints: "http://<ETCD_IP>:<ETCD_PORT>"#etcd_endpoints: "https://192.168.177.100:2379,https://192.168.177.101:2379,https://192.168.177.102:2379"#g' calico-etcd.yaml

# 替换ca
$ ETCD_CA=`cat /etc/kubernetes/pki/etcd/ca.crt | base64 | tr -d '\n'`
$ ETCD_CERT=`cat /etc/kubernetes/pki/etcd/server.crt | base64 | tr -d '\n'`
$ ETCD_KEY=`cat /etc/kubernetes/pki/etcd/server.key | base64 | tr -d '\n'`
$ sed -i "s@# etcd-key: null@etcd-key: ${ETCD_KEY}@g; s@# etcd-cert: null@etcd-cert: ${ETCD_CERT}@g; s@# etcd-ca: null@etcd-ca: ${ETCD_CA}@g" calico-etcd.yaml

$ sed -i 's#etcd_ca: ""#etcd_ca: "/calico-secrets/etcd-ca"#g; s#etcd_cert: ""#etcd_cert: "/calico-secrets/etcd-cert"#g; s#etcd_key: "" #etcd_key: "/calico-secrets/etcd-key" #g' calico-etcd.yaml

$ POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= '{print $NF}'`
 
$ sed -i 's@# - name: CALICO_IPV4POOL_CIDR@- name: CALICO_IPV4POOL_CIDR@g; s@#   value: "192.168.0.0/16"@  value: '"${POD_SUBNET}"'@g' calico-etcd.yaml

# 替换镜像地址
# 由于国内墙的原因，下载k8s所需的镜像经常会出现问题，可以先在每个节点用docker下载好指定的版本的镜像后再安装calico
# 示例：
$ sed -i 's#registry.cn-beijing.aliyuncs.com/dotbalo#calico#g' calico-etcd.yaml
sed -i 's#calico/cni:latest#calico/cni:v3.15.5#g' calico-etcd.yaml
sed -i 's#calico/kube-controllers:latest#calico/kube-controllers:v3.15.5#g' calico-etcd.yaml
sed -i 's#calico/node:latest#calico/node:v3.15.5#g' calico-etcd.yaml
sed -i 's#calico/pod2daemon-flexvol:latest#calico/pod2daemon-flexvol:v3.15.5#g' calico-etcd.yaml


# 安装
$ kubectl apply -f calico-etcd.yaml
```



### 部署Metrics Server

在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。

将master1节点的front-proxy-ca.crt复制到所有Node节点

```sh
scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt
scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt
```

安装metrics server

```sh
# 模板 https://github.com/dotbalo/k8s-ha-install/blob/manual-installation-v1.20.x/metrics-server-0.4.x-kubeadm/comp.yaml

# 安装
$ kubectl create -f comp.yaml

# 查看度量值标
$ kubectl top
```



### 部署Dashboard - Kuboard







### 扩展

#### 配置网络模式为ipvs

如果忘记配置集群网络模式，可以动态修改

```sh
# 在master1节点上执行
$ kubectl edit cm kube-proxy -n kube-system

# 滚动更新 kube-proxy 的 pod
$ kubectl patch daemonset kube-proxy -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"date\":\"`date + '%s'`\"}}}}}" -n kube-system

# 验证kube-proxy模式
curl 127.0.0.1:10249/proxyMode
```



#### master节点部署pod

master节点有污点，不能部署pod，在测试学习时为了节省资源，可以把master节点的污点去掉

```sh
# kubectl taint nodes node1 key:effect-
kubectl taint node -l node-role.kubernetes.io/master node-role.kubernetes.io/master:NoSchedule-

# 查看结果
curl 127.0.0.1:10249/proxyMode
```





### 安装异常处理

####  IPV6重复

ipv6 duplicate address detected

```sh
$ cp /etc/machine-id /etc/machine-id-bak
$ rm /etc/machine-id
$ systemd-machine-id-setup
```





## 以二进制的方式搭建集群

在集群容器规模大了以后，kubeadm搭建的集群会存在性能问题，kubeadm安装的集群，核心组件是以容器的方式运行，启动速度不够快